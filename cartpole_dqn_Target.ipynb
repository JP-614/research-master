{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7ccd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from glob import glob\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipy_display\n",
    "from gym import logger as gym_logger\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "#### show video func\n",
    "def show_video(mode='train', filename=None):\n",
    "    mp4_list = glob(mode+'/*.mp4')\n",
    "    # print(mp4_list)\n",
    "    if mp4_list:\n",
    "        if filename :\n",
    "            file_lists = glob(mode+'/'+filename)\n",
    "            if not file_lists:\n",
    "                print('No {} found'.format(filename))\n",
    "                return -1\n",
    "            mp4 = file_lists[0]\n",
    "                    \n",
    "        else:\n",
    "            mp4 = sorted(mp4_list)[-1]\n",
    "\n",
    "        print(mp4)\n",
    "        video = open(mp4, 'r+b').read()\n",
    "        encoded = b64encode(video)\n",
    "        ipy_display.display(HTML(data='''\n",
    "            <video alt=\"gameplay\" autoplay controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,%s\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "        ''' % (encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print('No video found')\n",
    "        return -1\n",
    "#### early stopping by avg\n",
    "class EarlyStopping_by_avg():\n",
    "    def __init__(self, patience=10, verbose=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.best_avg = 0\n",
    "        self.step = 0\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def check(self, avg , avg_scores):\n",
    "        ## best avg가 나올경우\n",
    "        if avg >= self.best_avg:\n",
    "            self.best_avg = avg\n",
    "            self.step = 0\n",
    "            # print(\"avg_reset\")\n",
    "        ## 이전값보다 현재 avg가 높을경우\n",
    "        elif len(avg_scores) > 1 and avg > avg_scores[-2]:  ### 이전 값과 비교해야하므로 -2  , -1은 지금 avg와 동일함\n",
    "            self.step = 0\n",
    "            # print(\"이전값보다 avg 높아서 reset\")\n",
    "        else:\n",
    "            self.step += 1\n",
    "            if self.step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('조기 종료')\n",
    "                return True\n",
    "        return False\n",
    "## dqn\n",
    "class DQN():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # 상태 및 행동 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # 감쇠율 및 엡실론 정의\n",
    "        # Greedy in the limit of infinite exploration (GLIE) 정책 고려\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # 리플레이 버퍼 크기 및 학습 시작 크기 정의\n",
    "        self.buffer_size = 2000\n",
    "        self.buffer_size_train_start = 200\n",
    "\n",
    "        # 리플레이 버퍼 정의\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "        # 인공신경망 학습 하이퍼파라미터 설정\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = Adam(learning_rate = self.learning_rate)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Q-네트워크 및 타겟(Q)-네트워크 정의\n",
    "        self.q_network = self.get_network()\n",
    "        self.target_q_network = self.get_network()\n",
    "\n",
    "        # 타겟(Q)-네트워크 파라미터 복제 함수 정의\n",
    "        self.update_target_network()\n",
    "        \n",
    "        ### check point 생성######\n",
    "        self.dir_name = os.getcwd()\n",
    "        self.folder_checkpoint = os.path.join(self.dir_name,'checkpoint')\n",
    "        self.checkpoint = tf.train.Checkpoint(model=self.q_network , optimizer=self.optimizer)\n",
    "        self.manager = tf.train.CheckpointManager(self.checkpoint, self.folder_checkpoint, max_to_keep=40)\n",
    "        \n",
    "\n",
    "    def update_target_network(self):\n",
    "        weights = self.q_network.get_weights()\n",
    "        self.target_q_network.set_weights(weights)\n",
    "\n",
    "    def get_network(self):\n",
    "        network = Sequential()\n",
    "        network.add(Dense(24, activation='relu', input_shape=(self.state_size,)))    # (None, 4) = (4,)\n",
    "        network.add(Dense(24, activation='relu'))\n",
    "        network.add(Dense(12, activation='relu'))\n",
    "        network.add(Dense(self.action_size))\n",
    "\n",
    "        return network\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 입력받은 상태, 행동, 보상, 다음상태, done flag를 리플레이 버퍼에 축적하는 함수 구현\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(item)\n",
    "    \n",
    "    def policy(self, state):\n",
    "        # 입력받은 상태에 대하여 행동을 결정하는 함수 구현\n",
    "        # 엡실론-그리디 알고리즘 구현\n",
    "\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            # random\n",
    "            action = np.random.choice([0,1])\n",
    "            # action = np.random.choice([x for x in range(self.action_size)])\n",
    "        else:\n",
    "            # greedy\n",
    "            # self.q_network.predict(state)\n",
    "            out = self.q_network(state)\n",
    "            # out = [Q[0], Q[1]]\n",
    "            action = np.argmax(out)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # GLIE 구현\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # 리플레이 버퍼에서 배치사이즈만큼 랜덤하게 추출하여 mini_batch 구현\n",
    "        # random.sample 함수 활용\n",
    "        mini_batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        # mini_batch에서 각 아래 정보로 분리하기\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
    "\n",
    "        # 분리된 정보를 tensor 형태로 변환\n",
    "        states = tf.convert_to_tensor(states)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        rewards = tf.convert_to_tensor(rewards)\n",
    "        next_states = tf.convert_to_tensor(next_states)\n",
    "        # dones를 True False로 바꿀 껀데 tf.float32 실수 형태로 바꿔 주는코드 (1.0 , 0.0)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # 학습 Target 정의\n",
    "        # r + gamma * max_q_target(next_s)\n",
    "        # mini_batch 단위로 elements-wise하게 계산되어야 합니다.\n",
    "        # max_q_target을 위해서 행렬 내 최대값을 반환해주는 np.amax 함수 활용 \n",
    "        # np.amax([[0.1, 0.9], [0.7, 0.3]], axis=-1))\n",
    "        # 에피소드의 마지막 상태의 경우, only 보상이 target 값이 된다.\n",
    "\n",
    "        ## 미래에 대한 target을 정하는 부분 --> 미래에 받을 가치 와 현재가치를 갖게 하면 현재의 가치를 최적의 상태로 만들 수 있음\n",
    "        q_next = self.target_q_network(next_states)\n",
    "        # print(\"q_next\", q_next)\n",
    "\n",
    "        ## 축별로 가장 높은값을 가져옴\n",
    "        max_q_next =  np.amax(q_next, axis=-1)\n",
    "        # print(\"max_q_next\", max_q_next)\n",
    "\n",
    "        ## 끝나지 않았을 경우 (done = False)\n",
    "        ## 끝났을 경우 (done = True) 의 경우 self.gamma * max_q_next 값을 포함하지 않아야함.\n",
    "        targets = rewards + ( self.gamma * max_q_next ) * ( 1-dones )\n",
    "\n",
    "        # print('targets',targets)\n",
    "\n",
    "        # q_network 학습\n",
    "        # tf.GradientTape() 함수를 활용하여 자동미분 후 학습에 활용\n",
    "        # (참조: https://teddylee777.github.io/tensorflow/gradient-tape)\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            q = self.q_network(states)\n",
    "            # print(actions)\n",
    "            one_hot_a = tf.one_hot( actions, self.action_size )\n",
    "\n",
    "            ## one hot encindg 값과 q값을 곱해준다.\n",
    "            q_sa = tf.reduce_sum(q * one_hot_a, axis=1)\n",
    "            # print(q_sa)\n",
    "            \n",
    "            # 오차 계산\n",
    "            loss = self.loss_fn(targets, q_sa)\n",
    "\n",
    "        # 손실함수를 통해 계산한 오차를 네트워크 가중치로 미분!\n",
    "        ## 여기서 q_network 만 학습을 진행함\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_weights)\n",
    "        # 미분값을 기준으로 각 네트워크 가중치를 업데이트!\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_weights))\n",
    "        \n",
    "        ### check point 저장하기\n",
    "        save_path = self.manager.save()\n",
    "        # print(\"Saved checkpoint {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6361924c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# 현재 상태에 대하여 행동 정의\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# env.step 함수를 이용하여 행동에 대한 다음 상태, 보상, done flag 등 획득\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# CartPole 환경 정의\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "# env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# 비디오 레코딩\n",
    "env = RecordVideo(env, './train', episode_trigger =lambda episode_number: True )\n",
    "env.metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "# CartPole 환경의 상태와 행동 크기 정의\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# 위에서 정의한 DQN 클래스를 활용하여 agent 정의\n",
    "agent = DQN(state_size, action_size)\n",
    "\n",
    "scores, avg_scores, episodes, losses = [], [], [], []\n",
    "\n",
    "# 반복 학습 에피소드 수 정의\n",
    "num_episode = 100\n",
    "early_stopping_by_avg = EarlyStopping_by_avg(patience=10, verbose=1)\n",
    "## early stopping을 위한 초기값 설정 \n",
    "avg_step = 0\n",
    "\n",
    "for epoch in range(num_episode):\n",
    "    # done flag와 score 값 초기화\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "\n",
    "    # 환경 reset을 통해 초기 상태 정의\n",
    "    state = env.reset()\n",
    "    \n",
    "    # print(f\"avg: {avg_step}\")\n",
    "    if early_stopping_by_avg.check(avg_step , avg_scores ):\n",
    "        print(\"earstpping 실행\")\n",
    "        break\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # 현재 상태에 대하여 행동 정의\n",
    "        action = agent.policy(state[np.newaxis,:])\n",
    "\n",
    "        # env.step 함수를 이용하여 행동에 대한 다음 상태, 보상, done flag 등 획득\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 해당 에피소드의 최종 score를 위해 reward 값 누적\n",
    "        score += reward\n",
    "\n",
    "        # 기본 환경은 pole이 쓰러지지 않으면 +1의 보상을 준다.\n",
    "        # 자신만의 보상가설을 만들어 학습 가능\n",
    "        ### 종료조건\n",
    "        # 폴 각도는 ±12° 이상입니다.\n",
    "        # 카트 위치가 ±2.4 이상(카트 중앙이 디스플레이 가장자리에 도달함)\n",
    "        # 에피소드 길이가 200보다 큽니다.\n",
    "        # TODO #\n",
    "        \n",
    "        def get_reward(pos, angle , done):\n",
    "            ### 위치 / 속도 조건으로 보상크게\n",
    "            cond_pos = (pos < 2.0) and (pos > -2.0)\n",
    "            cond_angle = (angle < 5.0) and (angle > -5.0)\n",
    "            ### 실패시 보상 -1\n",
    "            if done:\n",
    "                return -100.0\n",
    "            ### 상점\n",
    "            elif cond_pos or cond_angle:\n",
    "                return 0.1\n",
    "            elif cond_pos:\n",
    "                return 0.3\n",
    "            elif cond_pos and cond_angle:\n",
    "                return 0.5\n",
    "            ### 벌점\n",
    "            elif (pos > 2.5) or (pos < -2.5):\n",
    "                return -20\n",
    "            elif (angle > 10.0) or (angle < -10.0):\n",
    "                return -10\n",
    "            \n",
    "            \n",
    "        ### position\n",
    "        pos = next_state[0]\n",
    "        ### velocity\n",
    "        angle = next_state[2]\n",
    "        reward = get_reward(pos, angle, done)\n",
    "        # print(f'reward : {reward : .3f}')\n",
    "        # reward=0.1 if not done else -1\n",
    "\n",
    "\n",
    "        # 획득된 상태, 행동, 보상, 다음상태, done flag를 리플레이 버퍼에 축적\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # 다음 상태를 현재 상태로 정의\n",
    "        state = next_state\n",
    "\n",
    "        # buffer 크기가 일정 기준 이상 쌓이면 학습 진행\n",
    "        # TODO #\n",
    "        if len(agent.buffer) >= agent.buffer_size_train_start :\n",
    "            agent.train()\n",
    "\n",
    "        if done:\n",
    "            \n",
    "            ### early stop\n",
    "            avg_step = np.mean(scores[-10:])\n",
    "\n",
    "\n",
    "            # 에피소드가 종료되면 target_q_network 파라미터 복제\n",
    "            # TODO #\n",
    "            agent.update_target_network()\n",
    "\n",
    "            # 에피소드 종료마다 결과 그래프 저장\n",
    "            scores.append(score)\n",
    "            avg_scores.append(avg_step)\n",
    "            episodes.append(epoch)\n",
    "\n",
    "            \n",
    "            # 에피소드 종료마다 결과 출력\n",
    "            print(f'episode: {epoch:3d} | avg_score: { avg_step :3.2f} | buffer_size: {len(agent.buffer):4d} | epsilon: {agent.epsilon:.4f}')\n",
    "            \n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.title('Test graph')\n",
    "plt.xlabel('episodes')\n",
    "\n",
    "plt.plot(episodes, avg_scores,\n",
    "         color='skyblue',\n",
    "         marker='o', markerfacecolor='blue',\n",
    "         markersize=6)\n",
    "plt.ylabel('avg_scores', color='blue')\n",
    "plt.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "plt.savefig('cartpole_graph.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
